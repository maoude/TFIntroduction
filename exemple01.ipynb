{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools \n",
    "import collections\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) # the simplest way to create a dataset is to create it from a python list:\n",
    "for element in dataset: \n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])  # To process lines from files, use tf.data.TextLineDataset0\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "dataset = dataset.map(lambda x: x*2) #  Once you have a dataset, you can apply transformations to prepare the data for your model:\n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a Dataset of a step-separated range of values\n",
    "list(Dataset.range(5).as_numpy_iterator()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.range(2, 5).as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.range(1, 5, 2).as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.range(1, 5, -2).as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.range(5, 1, -2).as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.range(5, 1).as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduces the input dataset to a single element.\n",
    "tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy() \n",
    "#reduce(initial_state, reduce_func)\n",
    "#Reduces the input dataset to a single element.\n",
    "#The transformation calls reduce_func successively on every element \n",
    "#of the input dataset until the dataset is exhausted, \n",
    "#aggregating information in its internal state. \n",
    "#The initial_state argument is used for the initial state \n",
    "# and the final state is returned as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "dataset = dataset.repeat(3) \n",
    "list(dataset.as_numpy_iterator()) \n",
    "#Repeats this dataset so each original value is seen count times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ] \n",
    "dataset = dataset.map(lambda x: x + 1) \n",
    "#  Maps map_func across the elements of this dataset.\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.range(5) \n",
    "# `map_func` takes a single argument of type `tf.Tensor` with the same \n",
    "# shape and dtype. \n",
    "result = dataset.map(lambda x: x + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each element is a tuple containing two `tf.Tensor` objects. \n",
    "elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz)\")] \n",
    "dataset = tf.data.Dataset.from_generator( \n",
    "    lambda: elements, (tf.int32, tf.string)) \n",
    "# `map_func` takes two arguments of type `tf.Tensor`. This function \n",
    "# projects out just the first component. \n",
    "result = dataset.map(lambda x_int, y_str: x_int) \n",
    "list(result.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Point(x=1, y=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools \n",
    "import collections\n",
    "a = 1 # Integer element \n",
    "b = 2.0 # Float element \n",
    "c = (1, 2) # Tuple element with 2 components \n",
    "d = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components \n",
    "Point = collections.namedtuple(\"Point\", [\"x\", \"y\"]) # doctest: +SKIP Elements can be nested structures of tuples, named tuples, and dictionaries.\n",
    "e = Point(1, 2) # Named tuple # doctest: +SKIP \n",
    "f = tf.data.Dataset.range(10) # Dataset element \n",
    "e                  # readable __repr__ with a name=value style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0] + e[1] # indexable like the plain tuple (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = e                # unpack like a regular tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.x + e.y               # fields also accessible by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]).element_spec  # element_spec: The type specification of an element of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100) \n",
    "def dataset_fn(ds): # transformation_func: A function that takes one Dataset argument and returns a Dataset.\n",
    "  return ds.filter(lambda x: x < 5) \n",
    "dataset = dataset.apply(dataset_fn) # apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.\n",
    "list(dataset.as_numpy_iterator()) # Returns an iterator which converts all elements of the dataset to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "for element in dataset.as_numpy_iterator(): \n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "print(list(dataset.as_numpy_iterator())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]), \n",
    "                                              'b': [5, 6]}) \n",
    "list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5}, # as_numpy_iterator() will preserve the nested structure of dataset elements.\n",
    "                                      {'a': (2, 4), 'b': 6}] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2], dtype=int64),\n",
       " array([3, 4, 5], dtype=int64),\n",
       " array([6, 7], dtype=int64)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)  \n",
    "dataset = dataset.batch(3) # Combines consecutive elements of this dataset into batches.\n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2], dtype=int64), array([3, 4, 5], dtype=int64)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8) \n",
    "dataset = dataset.batch(3, drop_remainder=True) #  If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5) \n",
    "dataset = dataset.map(lambda x: x**2) \n",
    "dataset = dataset.cache() #Caches the elements in this dataset.\n",
    "# The first time reading through the data will generate the data using \n",
    "# `range` and `map`. \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsequent iterations read from the cache. \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ] \n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ] \n",
    "ds = a.concatenate(b) # Creates a Dataset by concatenating the given dataset with this dataset.\n",
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a 1D tensor produces scalar tensor elements. \n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a 1D tensor produces scalar tensor elements. \n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a tuple of 1D tensors produces tuple elements containing \n",
    "# scalar tensors. \n",
    "dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6])) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary structure is also preserved. \n",
    "dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]}) \n",
    "list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3}, \n",
    "                                      {'a': 2, 'b': 4}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 3],\n",
      "       [2, 3]]), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]]), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]]), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "# Two tensors can be combined into one Dataset object. \n",
    "features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor \n",
    "labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor \n",
    "dataset = dataset.from_tensor_slices((features, labels)) \n",
    "# Both the features and the labels tensors can be converted \n",
    "# to a Dataset object separately and combined after. \n",
    "features_dataset = dataset.from_tensor_slices(features) \n",
    "labels_dataset = dataset.from_tensor_slices(labels) \n",
    "dataset = dataset.zip((features_dataset, labels_dataset)) \n",
    "# A batched feature and label set can be converted to a Dataset \n",
    "# in similar fashion. \n",
    "batched_features = tf.constant([[[1, 3], [2, 3]], \n",
    "                                [[2, 1], [1, 2]], \n",
    "                                [[3, 3], [3, 2]]], shape=(3, 2, 2)) \n",
    "batched_labels = tf.constant([['A', 'A'], \n",
    "                              ['B', 'B'], \n",
    "                              ['A', 'B']], shape=(3, 2, 1)) \n",
    "dataset = dataset.from_tensor_slices((batched_features, batched_labels)) \n",
    "for element in dataset.as_numpy_iterator(): \n",
    "  print(element) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(6, 2)\n",
      "(7, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "dataset = dataset.enumerate(start=5) #Enumerates the elements of this dataset.\n",
    "for element in dataset.as_numpy_iterator(): \n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([7, 8]))\n",
      "(1, array([ 9, 10]))\n"
     ]
    }
   ],
   "source": [
    "# The nested structure of the input dataset determines the structure of \n",
    "# elements in the resulting dataset. \n",
    "dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)]) \n",
    "dataset = dataset.enumerate() \n",
    "for element in dataset.as_numpy_iterator(): \n",
    "  print(element) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \n",
    "dataset = dataset.filter(lambda x: x < 3) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `tf.math.equal(x, y)` is required for equality comparison \n",
    "def filter_fn(x): \n",
    "  return tf.math.equal(x, 1) \n",
    "dataset = dataset.filter(filter_fn) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.from_tensor_slices([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \n",
    "dataset = dataset.flat_map(lambda x: dataset.from_tensor_slices(x)) # Use flat_map if you want to make sure that the order of your dataset stays the same. \n",
    "#For example, to flatten a dataset of batches into a dataset of their elements\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, array([1], dtype=int64)),\n",
       " (2, array([1, 1], dtype=int64)),\n",
       " (3, array([1, 1, 1], dtype=int64))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen(): \n",
    "  for i in itertools.count(1): \n",
    "    yield (i, [1] * i) \n",
    "dataset = tf.data.Dataset.from_generator(gen,  #Creates a Dataset whose elements are generated by generator.\n",
    "     (tf.int64, tf.int64), \n",
    "     (tf.TensorShape([]), tf.TensorShape([None]))) \n",
    "list(dataset.take(3).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensors([1, 2, 3]) # Creates a Dataset with a single element, comprising the given tensors.\n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1, 2, 3]), b'A')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A')) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 4 files concurrently, and interleave blocks of 16 records \n",
    "# from each file. \n",
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\", \n",
    "             \"/var/data/file3.txt\", \"/var/data/file4.txt\"] \n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames) \n",
    "def parse_fn(filename): \n",
    "  return tf.data.Dataset.range(10) \n",
    "dataset = dataset.interleave(lambda x: \n",
    "    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1), \n",
    "    cycle_length=4, block_length=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ] \n",
    "# NOTE: New lines indicate \"block\" boundaries. \n",
    "dataset = dataset.interleave( \n",
    "    lambda x: dataset.from_tensors(x).repeat(6), \n",
    "    cycle_length=2, block_length=4) \n",
    "list(dataset.as_numpy_iterator()) \n",
    "#The cycle_length and block_length arguments control the order in which elements are produced. \n",
    "#cycle_length controls the number of input elements that are processed concurrently.\n",
    "#cycle_length input elements, open iterators on the returned Dataset objects, \n",
    "# and cycle through them producing block_length consecutive elements from each iterator, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0]\n",
      " [3 4 5]]\n",
      "[[6 7]\n",
      " [8 0]]\n"
     ]
    }
   ],
   "source": [
    "elements = [[1, 2], \n",
    "            [3, 4, 5], \n",
    "            [6, 7], \n",
    "            [8]] \n",
    "A = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32) \n",
    "# Pad to the smallest per-batch size that fits all elements. \n",
    "B = A.padded_batch(2, padded_shapes=[None]) \n",
    "for element in B.as_numpy_iterator(): \n",
    "  print(element) \n",
    "#Combines consecutive elements of this dataset into padded batches.\n",
    "#This transformation combines multiple consecutive elements of the input dataset into a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(3) \n",
    "dataset = dataset.prefetch(2) \n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.add(3, 5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 7, 16],\n",
       "       [10, 25]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[3, 5], [4, 8]])\n",
    "b = tf.constant([[1, 6], [2, 9]])\n",
    "tf.math.add_n([a, b, a])  # [[7, 16], [10, 25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "with tf.compat.v1.Session() as ses:\n",
    "     # Build a graph.\n",
    "     a = tf.constant(5.0)\n",
    "     b = tf.constant(6.0)\n",
    "     c = tf.add(a, b)\n",
    "     # Evaluate the tensor `c`.\n",
    "     print(ses.run(c))\n",
    "# Using the `close()` method.\n",
    "ses.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "op1 = tf.add(x, y)\n",
    "op2 = tf.multiply(x, y)\n",
    "op3 = tf.pow(op2, op1)\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    op3 = sess.run(op3)\n",
    "print(op3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "add_op = tf.add(x, y)\n",
    "mul_op = tf.multiply(x, y)\n",
    "useless = tf.multiply(x, add_op)\n",
    "pow_op = tf.pow(add_op, mul_op)\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    op3 = sess.run(pow_op)\n",
    "print(op3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_14:0\", shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try: \n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "except: \n",
    "  # Invalid device or cannot modify virtual devices once initialized. \n",
    "  pass \n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='b')\n",
    "  c = tf.multiply(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "#sess = tf.compat.v1.Session(config=tf.config.experimental(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
